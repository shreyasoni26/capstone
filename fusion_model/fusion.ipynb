{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1452529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LATE FUSION  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "# Suppress the FutureWarning related to is_copy/inplace operations\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "# Suppress the generic SettingWithCopyWarning\n",
    "# warnings.filterwarnings(\"ignore\", category=pd.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Blood Test Model (XGBoost) ---\n",
    "BLOOD_MODEL_PATH = 'xgb_cad_severity_model-7.joblib'\n",
    "SCALER_PATH = 'scaler_cad_severity-7.joblib'\n",
    "FEATURES_LIST_PATH = 'selected_features-7.joblib'\n",
    "DUCKDB_PATH = '../../final_db/mimic_analysis.db'\n",
    "\n",
    "# --- ECG Model (PyTorch) ---\n",
    "ECG_MODEL_PATH = './cnn_lstm_hybrid_checkpoints/hybrid_model_epoch60.pt' \n",
    "CLEANED_CSV_PATH = 'metadata_with_features.csv'\n",
    "\n",
    "FUSION_DIR = 'fusion_1_checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Shared Config ---\n",
    "BATCH_SIZE = 64\n",
    "LEAD_COUNT = 12\n",
    "SAMPLES = 2500\n",
    "NUM_CLASSES = 3 \n",
    "NUM_CLINICAL_FEATURES = 8 \n",
    "HIDDEN_SIZE = 64\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62fd9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1. ECG MODEL ARCHITECTURE (Copied from ECG script)\n",
    "# ============================================\n",
    "\n",
    "class CNNLSTM_FeatureExtractor(nn.Module):\n",
    "    \"\"\"Processes the raw ECG signal to generate a deep feature vector.\"\"\"\n",
    "    def __init__(self, hidden_size=HIDDEN_SIZE, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # CNN (Feature Extraction)\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(LEAD_COUNT, 32, kernel_size=15, stride=2, padding=7),\n",
    "            nn.BatchNorm1d(32), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=2, padding=1),\n",
    "            \n",
    "            nn.Conv1d(32, 64, kernel_size=11, stride=2, padding=5),\n",
    "            nn.BatchNorm1d(64), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=2, padding=1),\n",
    "            \n",
    "            nn.Conv1d(64, 128, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm1d(128), nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # LSTM (Temporal Context)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=128, hidden_size=hidden_size, num_layers=num_layers,\n",
    "            batch_first=True, bidirectional=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        cnn_out = self.cnn(x) \n",
    "        lstm_input = cnn_out.transpose(1, 2) \n",
    "        \n",
    "        _, (h_n, _) = self.lstm(lstm_input)\n",
    "        \n",
    "        # Concatenate final hidden states from both directions\n",
    "        final_state = torch.cat((h_n[-2, :, :], h_n[-1, :, :]), dim=1)\n",
    "        return final_state\n",
    "        \n",
    "class FinalMultiInputHybridModel(nn.Module):\n",
    "    \"\"\"Combines deep features (CNN-LSTM output) and 8 handcrafted features.\"\"\"\n",
    "    def __init__(self, num_classes=NUM_CLASSES, num_clinical_features=NUM_CLINICAL_FEATURES):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.signal_extractor = CNNLSTM_FeatureExtractor(hidden_size=HIDDEN_SIZE)\n",
    "        \n",
    "        # Input size: (2 * HIDDEN_SIZE for bidirectional LSTM) + NUM_CLINICAL_FEATURES\n",
    "        INPUT_SIZE = (2 * HIDDEN_SIZE) + num_clinical_features \n",
    "        \n",
    "        self.final_fc = nn.Sequential(\n",
    "            nn.Linear(INPUT_SIZE, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, signal, clinical_features):\n",
    "        deep_features = self.signal_extractor(signal) \n",
    "        \n",
    "        combined_features = torch.cat((deep_features, clinical_features.float()), dim=1) \n",
    "        \n",
    "        return self.final_fc(combined_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fc3d195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2. ECG DATASET CLASS (Copied from ECG script)\n",
    "# ============================================\n",
    "class HybridECGDataset(Dataset):\n",
    "    \"\"\"Dataset for loading ECG signal and 8 handcrafted features.\"\"\"\n",
    "    def __init__(self, df):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # --- Load Signal ---\n",
    "        try:\n",
    "            signal = np.load(row['processed_npy_path']).T\n",
    "            if signal.shape[1] > SAMPLES:\n",
    "                signal = signal[:, :SAMPLES]\n",
    "            elif signal.shape[1] < SAMPLES:\n",
    "                 # Pad if signal is too short\n",
    "                 padding = np.zeros((LEAD_COUNT, SAMPLES - signal.shape[1]), dtype=np.float32)\n",
    "                 signal = np.concatenate([signal, padding], axis=1)\n",
    "            signal = np.nan_to_num(signal, nan=0.0)\n",
    "        except Exception: \n",
    "            signal = np.zeros((LEAD_COUNT, SAMPLES), dtype=np.float32)\n",
    "\n",
    "        # --- Load Clinical Features ---\n",
    "        try:\n",
    "            features = np.load(row['feature_path'])\n",
    "        except Exception:\n",
    "            features = np.zeros(NUM_CLINICAL_FEATURES, dtype=np.float32)\n",
    "\n",
    "        label = {\"Low\": 0, \"Moderate\": 1, \"High\": 2}[row['severity_level']]\n",
    "        \n",
    "        # Return Signal, 8 Handcrafted Features, and Label\n",
    "        return (torch.tensor(signal, dtype=torch.float32), \n",
    "                torch.tensor(features, dtype=torch.float32)), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a631ad3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_align_test_data(cleaned_csv_path, duckdb_path):\n",
    "    \"\"\"\n",
    "    Loads ECG metadata and blood lab data, aligns blood features to the exact\n",
    "    feature order used in training (feature_names_full-7.joblib), and returns:\n",
    "      - X_blood_all_features : pd.DataFrame (N x 99) in exact training order\n",
    "      - X_ecg_metadata       : pd.DataFrame (contains processed_npy_path, feature_path, severity_level, etc.)\n",
    "      - y_test_fusion        : np.ndarray labels (0/1/2)\n",
    "      - original_feature_names_list : list of 99 feature names (order)\n",
    "    \"\"\"\n",
    "    import joblib\n",
    "    import pandas as pd\n",
    "    import duckdb\n",
    "    import numpy as np\n",
    "    import os\n",
    "\n",
    "    # 1) Load ECG metadata CSV\n",
    "    df_ecg_full = pd.read_csv(cleaned_csv_path)\n",
    "\n",
    "    # 2) Query DuckDB for admissions + labs (same logic as training)\n",
    "    con = duckdb.connect(duckdb_path)\n",
    "\n",
    "    df_blood_full = con.execute(\"\"\"\n",
    "        SELECT hadm_id, subject_id, anchor_age, gender, cad, severity_level\n",
    "        FROM admissions_severity\n",
    "        WHERE severity_level IS NOT NULL\n",
    "    \"\"\").df()\n",
    "\n",
    "    # lab item selection (same keyword groups as training)\n",
    "    cbc_keywords = [\"hemoglobin\", \"hematocrit\", \"rbc\", \"wbc\", \"platelet\", \"mcv\", \"mch\", \"mchc\", \"rdw\", \"neutrophil\", \"lymphocyte\", \"monocyte\", \"eosinophil\", \"basophil\"]\n",
    "    bmp_keywords = [\"sodium\", \"potassium\", \"chloride\", \"bicarbonate\", \"co2\", \"urea\", \"bun\", \"creatinine\", \"glucose\", \"calcium\"]\n",
    "    lft_keywords = [\"albumin\", \"protein\", \"bilirubin\", \"alkaline phosphatase\", \"ast\", \"sgot\", \"alt\", \"sgpt\", \"lactate\"]\n",
    "    lipid_keywords = [\"cholesterol\", \"hdl\", \"ldl\", \"triglyceride\"]\n",
    "    cardiac_keywords = [\"troponin\", \"ck-mb\", \"creatine kinase\", \"ck\", \"bnp\", \"nt-probnp\", \"hs-crp\"]\n",
    "    all_keywords = cbc_keywords + bmp_keywords + lft_keywords + lipid_keywords + cardiac_keywords\n",
    "    pattern = '|'.join(all_keywords)\n",
    "\n",
    "    df_labitems = con.execute(\"SELECT itemid, label, fluid FROM d_labitems WHERE LOWER(fluid) = 'blood'\").df()\n",
    "    mask = df_labitems['label'].str.lower().str.contains(pattern, na=False)\n",
    "    blood_itemids = df_labitems[mask]['itemid'].tolist()\n",
    "    if len(blood_itemids) == 0:\n",
    "        con.close()\n",
    "        raise RuntimeError(\"No matching blood lab items found. Check df_labitems and keyword list.\")\n",
    "\n",
    "    blood_itemids_str = ', '.join(map(str, blood_itemids))\n",
    "\n",
    "    df_labs = con.execute(f\"\"\"\n",
    "        SELECT l.subject_id, d.label, AVG(l.valuenum) as mean_value\n",
    "        FROM labevents l JOIN d_labitems d ON l.itemid = d.itemid\n",
    "        WHERE l.itemid IN ({blood_itemids_str})\n",
    "        GROUP BY l.subject_id, d.label\n",
    "    \"\"\").df()\n",
    "    df_labs_pivot = df_labs.pivot(index='subject_id', columns='label', values='mean_value').reset_index()\n",
    "\n",
    "    # Merge lab pivot with admission-level demographics\n",
    "    df_blood_full = pd.merge(df_blood_full, df_labs_pivot, on='subject_id', how='left')\n",
    "\n",
    "    # Add comorbidities\n",
    "    comorb_tables = {\n",
    "        'diabetes': 'diabetes_adm',\n",
    "        'hypertension': 'hypertension_adm',\n",
    "        'renal': 'renal_adm',\n",
    "        'obesity': 'obesity_icd_adm',\n",
    "        'smoker': 'smokers_adm'\n",
    "    }\n",
    "    # ensure hadm_id present as string\n",
    "    df_blood_full['hadm_id'] = df_blood_full['hadm_id'].astype(str)\n",
    "\n",
    "    for comorb, table in comorb_tables.items():\n",
    "        df_comorb = con.execute(f\"SELECT DISTINCT hadm_id, 1 AS {comorb} FROM {table}\").df()\n",
    "        if not df_comorb.empty:\n",
    "            df_comorb['hadm_id'] = df_comorb['hadm_id'].astype(str)\n",
    "            df_blood_full = pd.merge(df_blood_full, df_comorb, on='hadm_id', how='left')\n",
    "            df_blood_full[comorb] = df_blood_full[comorb].fillna(0).astype(int)\n",
    "        else:\n",
    "            # If table empty or absent, create column of zeros\n",
    "            df_blood_full[comorb] = 0\n",
    "\n",
    "    con.close()\n",
    "\n",
    "    # Guarantee expected comorb columns exist\n",
    "    expected_comorb_cols = list(comorb_tables.keys())\n",
    "    for col in expected_comorb_cols:\n",
    "        if col not in df_blood_full.columns:\n",
    "            df_blood_full[col] = 0\n",
    "\n",
    "    # Encode gender\n",
    "    df_blood_full['gender'] = df_blood_full['gender'].map({'M': 0, 'F': 1})\n",
    "    # If any unknown values remain, fill with median later\n",
    "\n",
    "    # Compute numeric medians (used to fill missing numeric values as a robust proxy)\n",
    "    numeric_cols = df_blood_full.select_dtypes(include=['number']).columns.tolist()\n",
    "    median_values = df_blood_full[numeric_cols].median().to_dict()\n",
    "\n",
    "    # Fill numeric NaNs with medians computed from df_blood_full\n",
    "    df_blood_full.fillna(median_values, inplace=True)\n",
    "\n",
    "    # ----- Load training artifacts (must exist in working directory) -----\n",
    "    try:\n",
    "        feature_names_full = joblib.load(\"feature_names_full-7.joblib\")   # list of 99 feature names used for scaler\n",
    "        selected_features = joblib.load(\"selected_features-7.joblib\")     # final selected 30 features\n",
    "        top_mi_features = joblib.load(\"top_mi_features-7.joblib\")         # optional debug list\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Required joblib artifact missing or unreadable: {e}\")\n",
    "\n",
    "    if not isinstance(feature_names_full, list):\n",
    "        feature_names_full = list(feature_names_full)\n",
    "\n",
    "    print(f\"Loaded artifacts: feature_names_full ({len(feature_names_full)}), selected_features ({len(selected_features)})\")\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Merge ECG metadata (paths) -> keep only patients with ECG metadata and severity labels\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # We only need subject_id + processed_npy_path + feature_path from ECG CSV\n",
    "    if 'processed_npy_path' not in df_ecg_full.columns or 'feature_path' not in df_ecg_full.columns:\n",
    "        raise RuntimeError(\"ECG metadata CSV must contain 'processed_npy_path' and 'feature_path' columns\")\n",
    "\n",
    "    df_common = pd.merge(\n",
    "        df_blood_full,\n",
    "        df_ecg_full[['subject_id', 'processed_npy_path', 'feature_path']],\n",
    "        on='subject_id', how='inner'\n",
    "    )\n",
    "    # Keep only the severity levels used in model training\n",
    "    df_common = df_common[df_common['severity_level'].isin(['Low', 'Moderate', 'High'])].reset_index(drop=True)\n",
    "\n",
    "    # Map severity to numeric label for stratification\n",
    "    df_common['severity_class'] = df_common['severity_level'].map({'Low': 0, 'Moderate': 1, 'High': 2})\n",
    "\n",
    "    # Train/test split to create an independent test set for fusion (same random state as training code)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    df_train, df_test_fusion = train_test_split(\n",
    "        df_common,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=df_common['severity_class']\n",
    "    )\n",
    "\n",
    "    # Ensure comorbidity columns exist in test\n",
    "    for col in expected_comorb_cols:\n",
    "        if col not in df_test_fusion.columns:\n",
    "            df_test_fusion[col] = 0\n",
    "\n",
    "    # Prepare the final blood matrix: drop identifiers and ECG columns\n",
    "    columns_to_drop_final = ['subject_id', 'hadm_id', 'cad', 'severity_level', 'severity_class', 'processed_npy_path', 'feature_path']\n",
    "    X_blood_all_features = df_test_fusion.drop(columns=[c for c in columns_to_drop_final if c in df_test_fusion.columns], errors='ignore')\n",
    "\n",
    "    # Reindex to exactly the training 99-feature order; missing columns become NaN\n",
    "    X_blood_all_features = X_blood_all_features.reindex(columns=feature_names_full)\n",
    "\n",
    "    # Fill any remaining NaNs with the numeric medians computed earlier (or 0 fallback)\n",
    "    for col in feature_names_full:\n",
    "        if col not in X_blood_all_features.columns:\n",
    "            # safety: add column of zeros (shouldn't happen because reindex added them)\n",
    "            X_blood_all_features[col] = 0\n",
    "    # Now replace NaNs with median_values (for numeric columns) or 0\n",
    "    for col in X_blood_all_features.columns:\n",
    "        if X_blood_all_features[col].isna().any():\n",
    "            if col in median_values:\n",
    "                X_blood_all_features[col].fillna(median_values[col], inplace=True)\n",
    "            else:\n",
    "                # Non-numeric or missing median - fill with 0\n",
    "                X_blood_all_features[col].fillna(0, inplace=True)\n",
    "\n",
    "    # Final sanity checks\n",
    "    print(f\"Blood test features shape: {X_blood_all_features.shape}\")\n",
    "    if X_blood_all_features.shape[1] != len(feature_names_full):\n",
    "        raise RuntimeError(f\"Feature count mismatch after reindexing: expected {len(feature_names_full)}, got {X_blood_all_features.shape[1]}\")\n",
    "\n",
    "    # Prepare outputs\n",
    "    y_test_fusion = df_test_fusion['severity_class'].values\n",
    "    X_ecg_metadata = df_test_fusion  # contains processed_npy_path, feature_path, severity_level etc.\n",
    "    original_feature_names_list = feature_names_full\n",
    "\n",
    "    return X_blood_all_features, X_ecg_metadata, y_test_fusion, original_feature_names_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1cb7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4. PREDICTION GENERATION\n",
    "# ============================================\n",
    "\n",
    "def generate_predictions(X_blood_raw, X_ecg_metadata, y_true, original_feature_names):\n",
    "    \"\"\"Generates P_A and P_B and creates the fusion dataset.\"\"\"\n",
    "    \n",
    "    print(\"Generating Predictions\")\n",
    "    print(f\"Original Feature List Size: {len(original_feature_names)}\")\n",
    "    \n",
    "    # --- Load XGBoost Components (Model A) ---\n",
    "    model_blood = joblib.load(BLOOD_MODEL_PATH)\n",
    "    scaler = joblib.load(SCALER_PATH)\n",
    "    selected_features = joblib.load(FEATURES_LIST_PATH)\n",
    "    \n",
    "    # --- Load ECG Model (Model B) ---\n",
    "    model_ecg = FinalMultiInputHybridModel(num_clinical_features=NUM_CLINICAL_FEATURES).to(DEVICE)\n",
    "    model_ecg.load_state_dict(torch.load(ECG_MODEL_PATH, map_location=DEVICE))\n",
    "    model_ecg.eval()\n",
    "    \n",
    "    # --- A. Model A: XGBoost Predictions (P_A) ---\n",
    "    X_test_blood_df = X_blood_raw.copy()\n",
    "    \n",
    "    # Get the names of the 99 features the scaler expects (passed from data loading)\n",
    "    expected_99_features = set(original_feature_names)\n",
    "    \n",
    "    # Get the names of the 104 features currently in the test data\n",
    "    current_104_features = set(X_test_blood_df.columns)\n",
    "    \n",
    "    # Find columns that are in the test data but NOT in the expected 99 features\n",
    "    extra_columns_to_drop = list(current_104_features - expected_99_features)\n",
    "    \n",
    "    if extra_columns_to_drop:\n",
    "        print(\"\\nFeature Mismatch Detected!\")\n",
    "        print(f\"Expected Features: {len(expected_99_features)}\")\n",
    "        print(f\"Received Features: {len(current_104_features)}\")\n",
    "        print(f\"Extra Columns Found (must be dropped): {extra_columns_to_drop}\")\n",
    "        \n",
    "        # Drop the extra columns\n",
    "        X_test_blood_df = X_test_blood_df.drop(columns=extra_columns_to_drop)\n",
    "        print(f\"Successfully dropped {len(extra_columns_to_drop)} columns.\")\n",
    "    \n",
    "    # Check for missing columns (should be empty if data loading is correct)\n",
    "    missing_columns = list(expected_99_features - set(X_test_blood_df.columns))\n",
    "    if missing_columns:\n",
    "        print(f\"FATAL ERROR: Missing columns required by scaler: {missing_columns}\")\n",
    "        return None, None # Stop execution if features are missing!\n",
    "    \n",
    "    # 1. Align the columns and order them correctly for the scaler (99 features)\n",
    "    # This step is crucial because the scaler relies on positional ordering.\n",
    "    X_test_full = X_test_blood_df[original_feature_names]\n",
    "    \n",
    "    # Forcing NumPy conversion ensures no residual Pandas metadata confuses the scaler.\n",
    "    X_test_full_np = X_test_full.values \n",
    "    \n",
    "    # DEBUG CHECK: What is the size immediately before the crash?\n",
    "    print(f\"DEBUG CHECK: X_test_full_np shape: {X_test_full_np.shape}\")\n",
    "    \n",
    "    # 2. Scale the full 99 features\n",
    "    X_test_scaled_full = scaler.transform(X_test_full_np)      #failing but why\n",
    "    \n",
    "    # 3. Select the final 30 features from the scaled array for the XGBoost model\n",
    "    \n",
    "    X_test_scaled_df = pd.DataFrame(X_test_scaled_full, columns=original_feature_names)\n",
    "    X_test_sel = X_test_scaled_df[selected_features]\n",
    "    \n",
    "    # Predict probabilities\n",
    "    P_A = model_blood.predict_proba(X_test_sel)\n",
    "    print(f\"P_A (XGBoost) probabilities shape: {P_A.shape}\")\n",
    "\n",
    "    # --- B. Model B: CNN-LSTM Predictions (P_B) ---\n",
    "    ecg_dataset = HybridECGDataset(X_ecg_metadata)\n",
    "    ecg_loader = DataLoader(ecg_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    P_B_list = []\n",
    "    with torch.no_grad():\n",
    "        for (x_signal, x_features), _ in tqdm(ecg_loader, desc=\"ECG Model Prediction\"):\n",
    "            x_signal, x_features = x_signal.to(DEVICE), x_features.to(DEVICE)\n",
    "            logits = model_ecg(x_signal, x_features)\n",
    "            probabilities = F.softmax(logits, dim=1).cpu().numpy()\n",
    "            P_B_list.append(probabilities)\n",
    "            \n",
    "    P_B = np.concatenate(P_B_list, axis=0)\n",
    "    print(f\"P_B (ECG-LSTM) probabilities shape: {P_B.shape}\")\n",
    "    \n",
    "    # --- C. Create Fusion Dataset ---\n",
    "    X_fusion = np.hstack((P_A, P_B)) # Shape N x 6\n",
    "    Y_fusion = y_true\n",
    "    \n",
    "    return X_fusion, Y_fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1a7e9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 5. LATE FUSION (META-MODEL TRAINING)\n",
    "# ============================================\n",
    "\n",
    "def perform_late_fusion(X_fusion, Y_fusion):\n",
    "    \"\"\"Trains a Logistic Regression Meta-Model on the combined probabilities.\"\"\"\n",
    "    print(\"\\nTraining Logistic Regression Meta-Model\")\n",
    "    \n",
    "    # Splitting the test set predictions into a Meta-Train and Meta-Test set\n",
    "    # This allows us to train the fusion rule and evaluate the final system fairly.\n",
    "    X_meta_train, X_meta_test, y_meta_train, y_meta_test = train_test_split(\n",
    "        X_fusion, Y_fusion, test_size=0.5, random_state=42, stratify=Y_fusion\n",
    "    )\n",
    "\n",
    "    # Train the Meta-Model (Logistic Regression)\n",
    "    meta_model = LogisticRegression(solver='liblinear', multi_class='auto', random_state=42)\n",
    "    meta_model.fit(X_meta_train, y_meta_train)\n",
    "    \n",
    "    # --- SAVE THE FUSION MODEL CHECKPOINT ---\n",
    "    \n",
    "    # 1. Create the directory if it doesn't exist\n",
    "    os.makedirs(FUSION_DIR, exist_ok=True)\n",
    "    \n",
    "    # 2. Define the full path for the checkpoint file\n",
    "    FUSION_MODEL_FILENAME = os.path.join(FUSION_DIR, 'fusion_meta_model.joblib')\n",
    "    \n",
    "    try:\n",
    "        joblib.dump(meta_model, FUSION_MODEL_FILENAME)\n",
    "        print(f\"\\nFusion Meta-Model successfully saved to {FUSION_MODEL_FILENAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving fusion model: {e}\")\n",
    "\n",
    "    # Evaluate Fusion Performance on the Meta-Test Set\n",
    "    y_fusion_pred = meta_model.predict(X_meta_test)\n",
    "    \n",
    "    acc_fusion = accuracy_score(y_meta_test, y_fusion_pred)\n",
    "    f1_w = f1_score(y_meta_test, y_fusion_pred, average='weighted')\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"FINAL LATE FUSION PERFORMANCE (Meta-Model)\")\n",
    "    print(f\"Accuracy on Meta-Test Set: {acc_fusion:.4f}\")\n",
    "    print(f\"Weighted F1-Score: {f1_w:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_meta_test, y_fusion_pred, target_names=['Low','Moderate','High']))\n",
    "    print('\\n')\n",
    "\n",
    "    # Optionally, you can also test the individual model performance on the same set for comparison\n",
    "    P_A_test = X_meta_test[:, 0:NUM_CLASSES]\n",
    "    P_B_test = X_meta_test[:, NUM_CLASSES:2*NUM_CLASSES]\n",
    "    \n",
    "    acc_a = accuracy_score(y_meta_test, np.argmax(P_A_test, axis=1))\n",
    "    acc_b = accuracy_score(y_meta_test, np.argmax(P_B_test, axis=1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6cd9b2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fusion\n",
      "Loaded artifacts: feature_names_full (99), selected_features (30)\n",
      "Blood test features shape: (11393, 99)\n",
      "Generating Predictions\n",
      "Original Feature List Size: 99\n",
      "DEBUG CHECK: X_test_full_np shape: (11393, 99)\n",
      "P_A (XGBoost) probabilities shape: (11393, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ECG Model Prediction: 100%|██████████| 179/179 [05:00<00:00,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_B (ECG-LSTM) probabilities shape: (11393, 3)\n",
      "\n",
      "Training Logistic Regression Meta-Model\n",
      "\n",
      "Fusion Meta-Model successfully saved to fusion_1_checkpoints\\fusion_meta_model.joblib\n",
      "\n",
      "\n",
      "FINAL LATE FUSION PERFORMANCE (Meta-Model)\n",
      "Accuracy on Meta-Test Set: 0.9326\n",
      "Weighted F1-Score: 0.9332\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Low       0.81      0.89      0.85       798\n",
      "    Moderate       0.95      0.94      0.94      3068\n",
      "        High       0.97      0.94      0.95      1831\n",
      "\n",
      "    accuracy                           0.93      5697\n",
      "   macro avg       0.91      0.92      0.92      5697\n",
      "weighted avg       0.93      0.93      0.93      5697\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#main exec\n",
    "print(f\"Starting fusion\")\n",
    "\n",
    "# Step 1: Load and Align the Common Test Data, Successfully loads the 99 feature names list\n",
    "X_blood_raw, X_ecg_metadata, y_test_fusion, original_feature_names_list = load_and_align_test_data(CLEANED_CSV_PATH, DUCKDB_PATH)\n",
    "\n",
    "# Step 2: Generate Predictions (P_A and P_B)\n",
    "X_fusion, Y_fusion = generate_predictions(X_blood_raw, X_ecg_metadata, y_test_fusion, original_feature_names_list)\n",
    "\n",
    "# Check if prediction generation was successful before fusion\n",
    "if X_fusion is not None:\n",
    "    perform_late_fusion(X_fusion, Y_fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15e67b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import duckdb\n",
    "# import os\n",
    "\n",
    "# DB_PATH = \"../../final_db/mimic_analysis.db\"\n",
    "\n",
    "# try:\n",
    "#     # 1. Attempt to connect to the locked database file path\n",
    "#     # This often forces DuckDB to acknowledge the old lock or cleanup resources.\n",
    "#     cleanup_con = duckdb.connect(DB_PATH) \n",
    "    \n",
    "#     # 2. Immediately close the connection in a controlled way\n",
    "#     cleanup_con.close()\n",
    "    \n",
    "#     print(f\"Successfully connected to and closed the connection to: {DB_PATH}\")\n",
    "#     print(\"The file lock should now be released.\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"Failed to connect and close the database at {DB_PATH}.\")\n",
    "#     print(f\"Error: {e}\")\n",
    "#     # In rare cases, you might need to restart your terminal/IDE or the machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b772555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import duckdb\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "# from tqdm import tqdm\n",
    "# import os\n",
    "# import warnings\n",
    "# import sys \n",
    "\n",
    "# # Suppress warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "# warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# # --- CONFIGURATION ---\n",
    "# BLOOD_MODEL_PATH = 'xgb_cad_severity_model-7.joblib'\n",
    "# SCALER_PATH = 'scaler_cad_severity-7.joblib'\n",
    "# FEATURES_LIST_PATH = 'selected_features-7.joblib'\n",
    "# DUCKDB_PATH = '../../final_db/mimic_analysis.db'\n",
    "\n",
    "# ECG_MODEL_PATH = './cnn_lstm_hybrid_checkpoints/hybrid_model_epoch60.pt' \n",
    "# CLEANED_CSV_PATH = 'metadata_with_features.csv'\n",
    "\n",
    "# FUSION_DIR = 'fusion_1_checkpoints'\n",
    "\n",
    "# BATCH_SIZE = 64\n",
    "# LEAD_COUNT = 12\n",
    "# SAMPLES = 2500\n",
    "# NUM_CLASSES = 3 \n",
    "# NUM_CLINICAL_FEATURES = 8 \n",
    "# HIDDEN_SIZE = 64\n",
    "# DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# # ============================================\n",
    "# # 1. ECG MODEL ARCHITECTURE \n",
    "# # ============================================\n",
    "\n",
    "# class CNNLSTM_FeatureExtractor(nn.Module):\n",
    "#     \"\"\"Processes the raw ECG signal to generate a deep feature vector.\"\"\"\n",
    "#     def __init__(self, hidden_size=HIDDEN_SIZE, num_layers=1):\n",
    "#         super().__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "        \n",
    "#         # CNN (Feature Extraction)\n",
    "#         self.cnn = nn.Sequential(\n",
    "#             nn.Conv1d(LEAD_COUNT, 32, kernel_size=15, stride=2, padding=7),\n",
    "#             nn.BatchNorm1d(32), nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool1d(kernel_size=3, stride=2, padding=1),\n",
    "            \n",
    "#             nn.Conv1d(32, 64, kernel_size=11, stride=2, padding=5),\n",
    "#             nn.BatchNorm1d(64), nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool1d(kernel_size=3, stride=2, padding=1),\n",
    "            \n",
    "#             nn.Conv1d(64, 128, kernel_size=7, stride=2, padding=3),\n",
    "#             nn.BatchNorm1d(128), nn.ReLU(inplace=True)\n",
    "#         )\n",
    "        \n",
    "#         # LSTM (Temporal Context)\n",
    "#         self.lstm = nn.LSTM(\n",
    "#             input_size=128, hidden_size=hidden_size, num_layers=num_layers,\n",
    "#             batch_first=True, bidirectional=True\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         cnn_out = self.cnn(x) \n",
    "#         lstm_input = cnn_out.transpose(1, 2) \n",
    "        \n",
    "#         _, (h_n, _) = self.lstm(lstm_input)\n",
    "        \n",
    "#         # Concatenate final hidden states from both directions\n",
    "#         final_state = torch.cat((h_n[-2, :, :], h_n[-1, :, :]), dim=1)\n",
    "#         return final_state\n",
    "        \n",
    "# class FinalMultiInputHybridModel(nn.Module):\n",
    "#     \"\"\"Combines deep features (CNN-LSTM output) and 8 handcrafted features.\"\"\"\n",
    "#     def __init__(self, num_classes=NUM_CLASSES, num_clinical_features=NUM_CLINICAL_FEATURES):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.signal_extractor = CNNLSTM_FeatureExtractor(hidden_size=HIDDEN_SIZE)\n",
    "        \n",
    "#         # Input size: (2 * HIDDEN_SIZE for bidirectional LSTM) + NUM_CLINICAL_FEATURES\n",
    "#         INPUT_SIZE = (2 * HIDDEN_SIZE) + num_clinical_features \n",
    "        \n",
    "#         self.final_fc = nn.Sequential(\n",
    "#             nn.Linear(INPUT_SIZE, 64),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(64, num_classes)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, signal, clinical_features):\n",
    "#         deep_features = self.signal_extractor(signal) \n",
    "        \n",
    "#         combined_features = torch.cat((deep_features, clinical_features.float()), dim=1) \n",
    "        \n",
    "#         return self.final_fc(combined_features)\n",
    "\t\t\n",
    "# # ============================================\n",
    "# # 2. ECG DATASET CLASS \n",
    "# # ============================================\n",
    "# class HybridECGDataset(Dataset):\n",
    "#     \"\"\"Dataset for loading ECG signal and 8 handcrafted features.\"\"\"\n",
    "#     def __init__(self, df):\n",
    "#         self.df = df.reset_index(drop=True)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         row = self.df.iloc[idx]\n",
    "        \n",
    "#         # --- Load Signal ---\n",
    "#         try:\n",
    "#             signal = np.load(row['processed_npy_path']).T\n",
    "#             if signal.shape[1] > SAMPLES:\n",
    "#                 signal = signal[:, :SAMPLES]\n",
    "#             elif signal.shape[1] < SAMPLES:\n",
    "#                  # Pad if signal is too short\n",
    "#                  padding = np.zeros((LEAD_COUNT, SAMPLES - signal.shape[1]), dtype=np.float32)\n",
    "#                  signal = np.concatenate([signal, padding], axis=1)\n",
    "#             signal = np.nan_to_num(signal, nan=0.0)\n",
    "#         except Exception: \n",
    "#             signal = np.zeros((LEAD_COUNT, SAMPLES), dtype=np.float32)\n",
    "\n",
    "#         # --- Load Clinical Features ---\n",
    "#         try:\n",
    "#             features = np.load(row['feature_path'])\n",
    "#         except Exception:\n",
    "#             features = np.zeros(NUM_CLINICAL_FEATURES, dtype=np.float32)\n",
    "\n",
    "#         label = {\"Low\": 0, \"Moderate\": 1, \"High\": 2}[row['severity_level']]\n",
    "        \n",
    "#         # Return Signal, 8 Handcrafted Features, and Label\n",
    "#         return (torch.tensor(signal, dtype=torch.float32), \n",
    "#                 torch.tensor(features, dtype=torch.float32)), label\n",
    "\t\t\t\t\n",
    "# # ============================================\n",
    "# # 3. DATA ACQUISITION & ALIGNMENT\n",
    "# # ============================================\n",
    "\n",
    "# def load_and_align_test_data(cleaned_csv_path, duckdb_path):\n",
    "#     \"\"\"\n",
    "#     Loads ECG metadata and blood lab data, aligns blood features to the exact\n",
    "#     99-feature schema used in training, and returns the test set components.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # 1) Load ECG metadata CSV\n",
    "#     df_ecg_full = pd.read_csv(cleaned_csv_path)\n",
    "\n",
    "#     # 2) Query DuckDB for admissions + labs (same logic as training)\n",
    "#     con = duckdb.connect(duckdb_path)\n",
    "\n",
    "#     df_blood_full = con.execute(\"\"\"\n",
    "#         SELECT hadm_id, subject_id, anchor_age, gender, cad, severity_level\n",
    "#         FROM admissions_severity\n",
    "#         WHERE severity_level IS NOT NULL\n",
    "#     \"\"\").df()\n",
    "\n",
    "#     # lab item selection (same keyword groups as training)\n",
    "#     cbc_keywords = [\"hemoglobin\", \"hematocrit\", \"rbc\", \"wbc\", \"platelet\", \"mcv\", \"mch\", \"mchc\", \"rdw\", \"neutrophil\", \"lymphocyte\", \"monocyte\", \"eosinophil\", \"basophil\"]\n",
    "#     bmp_keywords = [\"sodium\", \"potassium\", \"chloride\", \"bicarbonate\", \"co2\", \"urea\", \"bun\", \"creatinine\", \"glucose\", \"calcium\"]\n",
    "#     lft_keywords = [\"albumin\", \"protein\", \"bilirubin\", \"alkaline phosphatase\", \"ast\", \"sgot\", \"alt\", \"sgpt\", \"lactate\"]\n",
    "#     lipid_keywords = [\"cholesterol\", \"hdl\", \"ldl\", \"triglyceride\"]\n",
    "#     cardiac_keywords = [\"troponin\", \"ck-mb\", \"creatine kinase\", \"ck\", \"bnp\", \"nt-probnp\", \"hs-crp\"]\n",
    "#     all_keywords = cbc_keywords + bmp_keywords + lft_keywords + lipid_keywords + cardiac_keywords\n",
    "#     pattern = '|'.join(all_keywords)\n",
    "\n",
    "#     df_labitems = con.execute(\"SELECT itemid, label, fluid FROM d_labitems WHERE LOWER(fluid) = 'blood'\").df()\n",
    "#     mask = df_labitems['label'].str.lower().str.contains(pattern, na=False)\n",
    "#     blood_itemids = df_labitems[mask]['itemid'].tolist()\n",
    "    \n",
    "#     if len(blood_itemids) == 0:\n",
    "#         con.close()\n",
    "#         raise RuntimeError(\"No matching blood lab items found. Check df_labitems and keyword list.\")\n",
    "    \n",
    "#     blood_itemids_str = ', '.join(map(str, blood_itemids))\n",
    "\n",
    "#     df_labs = con.execute(f\"\"\"\n",
    "#         SELECT l.subject_id, d.label, AVG(l.valuenum) as mean_value\n",
    "#         FROM labevents l JOIN d_labitems d ON l.itemid = d.itemid\n",
    "#         WHERE l.itemid IN ({blood_itemids_str})\n",
    "#         GROUP BY l.subject_id, d.label\n",
    "#     \"\"\").df()\n",
    "#     df_labs_pivot = df_labs.pivot(index='subject_id', columns='label', values='mean_value').reset_index()\n",
    "\n",
    "#     # Merge lab pivot with admission-level demographics\n",
    "#     df_blood_full = pd.merge(df_blood_full, df_labs_pivot, on='subject_id', how='left')\n",
    "\n",
    "#     # Add comorbidities\n",
    "#     comorb_tables = {\n",
    "#         'diabetes': 'diabetes_adm',\n",
    "#         'hypertension': 'hypertension_adm',\n",
    "#         'renal': 'renal_adm',\n",
    "#         'obesity': 'obesity_icd_adm',\n",
    "#         'smoker': 'smokers_adm'\n",
    "#     }\n",
    "#     # ensure hadm_id present as string\n",
    "#     df_blood_full['hadm_id'] = df_blood_full['hadm_id'].astype(str)\n",
    "\n",
    "#     for comorb, table in comorb_tables.items():\n",
    "#         df_comorb = con.execute(f\"SELECT DISTINCT hadm_id, 1 AS {comorb} FROM {table}\").df()\n",
    "#         if not df_comorb.empty:\n",
    "#             df_comorb['hadm_id'] = df_comorb['hadm_id'].astype(str)\n",
    "#             df_blood_full = pd.merge(df_blood_full, df_comorb, on='hadm_id', how='left')\n",
    "#             df_blood_full[comorb] = df_blood_full[comorb].fillna(0).astype(int)\n",
    "#         else:\n",
    "#             # If table empty or absent, create column of zeros\n",
    "#             df_blood_full[comorb] = 0\n",
    "\n",
    "#     con.close()\n",
    "\n",
    "#     # Guarantee expected comorb columns exist (Post-merge cleanup)\n",
    "#     expected_comorb_cols = list(comorb_tables.keys())\n",
    "#     for col in expected_comorb_cols:\n",
    "#         if col not in df_blood_full.columns:\n",
    "#             df_blood_full[col] = 0\n",
    "\n",
    "#     # Encode gender\n",
    "#     df_blood_full['gender'] = df_blood_full['gender'].map({'M': 0, 'F': 1})\n",
    "    \n",
    "#     # Compute numeric medians (used to fill missing numeric values as a robust proxy)\n",
    "#     numeric_cols = df_blood_full.select_dtypes(include=['number']).columns.tolist()\n",
    "#     median_values = df_blood_full[numeric_cols].median().to_dict()\n",
    "\n",
    "#     # Fill numeric NaNs with medians computed from df_blood_full\n",
    "#     df_blood_full.fillna(median_values, inplace=True)\n",
    "\n",
    "#     # ----- Load training artifacts -----\n",
    "#     try:\n",
    "#         feature_names_full = joblib.load(f\"{SCALER_PATH[:-7]}feature_names_full-7.joblib\")   # list of 99 feature names used for scaler\n",
    "#         selected_features = joblib.load(FEATURES_LIST_PATH)     # final selected 30 features\n",
    "#     except Exception as e:\n",
    "#         raise RuntimeError(f\"Required joblib artifact missing or unreadable: {e}\")\n",
    "\n",
    "#     if not isinstance(feature_names_full, list):\n",
    "#         feature_names_full = list(feature_names_full)\n",
    "\n",
    "#     # --------------------------------------------------------------------------------\n",
    "#     # FIX: ESTABLISH GROUND TRUTH 99-FEATURE LIST \n",
    "#     # --------------------------------------------------------------------------------\n",
    "    \n",
    "#     columns_to_drop_for_X = ['subject_id', 'hadm_id', 'cad', 'severity_level', 'severity_class'] \n",
    "    \n",
    "#     X_initial_structure = df_blood_full.drop(\n",
    "#         columns=[col for col in columns_to_drop_for_X if col in df_blood_full.columns]\n",
    "#     )\n",
    "    \n",
    "#     # Store the list of 99 feature names derived from the initial training data structure\n",
    "#     original_feature_names_list = X_initial_structure.columns.tolist() \n",
    "    \n",
    "#     # --------------------------------------------------------------------------------\n",
    "    \n",
    "#     # Merge ECG metadata (paths) -> keep only patients with ECG metadata and severity labels\n",
    "#     df_common = pd.merge(\n",
    "#         df_blood_full,\n",
    "#         df_ecg_full[['subject_id', 'processed_npy_path', 'feature_path']],\n",
    "#         on='subject_id', how='inner'\n",
    "#     )\n",
    "    \n",
    "#     # Keep only the severity levels used in model training\n",
    "#     df_common = df_common[df_common['severity_level'].isin(['Low', 'Moderate', 'High'])].reset_index(drop=True)\n",
    "\n",
    "#     # Map severity to numeric label for stratification\n",
    "#     df_common['severity_class'] = df_common['severity_level'].map({'Low': 0, 'Moderate': 1, 'High': 2})\n",
    "\n",
    "#     # Train/test split to create an independent test set for fusion (same random state as training code)\n",
    "#     df_train, df_test_fusion = train_test_split(\n",
    "#         df_common,\n",
    "#         test_size=0.2,\n",
    "#         random_state=42,\n",
    "#         stratify=df_common['severity_class']\n",
    "#     )\n",
    "\n",
    "#     # Final check to ensure comorbidity columns exist in test\n",
    "#     for col in expected_comorb_cols:\n",
    "#         if col not in df_test_fusion.columns:\n",
    "#             df_test_fusion[col] = 0\n",
    "\n",
    "#     # Prepare the final blood matrix: drop identifiers and ECG columns\n",
    "#     columns_to_drop_final = ['subject_id', 'hadm_id', 'cad', 'severity_level', 'severity_class', 'processed_npy_path', 'feature_path']\n",
    "#     X_blood_all_features = df_test_fusion.drop(columns=[c for c in columns_to_drop_final if c in df_test_fusion.columns], errors='ignore')\n",
    "\n",
    "#     # Reindex to exactly the training 99-feature order; this line enforces the correct schema and order\n",
    "#     X_blood_all_features = X_blood_all_features[original_feature_names_list]\n",
    "\n",
    "#     # Final sanity check on the data size before returning (Should print 99)\n",
    "#     print(f\"Blood test features shape: {X_blood_all_features.shape}\")\n",
    "#     print(f\"Expected 99 features, got {X_blood_all_features.shape[1]} features\")\n",
    "\n",
    "#     # Prepare outputs\n",
    "#     y_test_fusion = df_test_fusion['severity_class'].values\n",
    "#     X_ecg_metadata = df_test_fusion  # contains processed_npy_path, feature_path, severity_level etc.\n",
    "    \n",
    "#     return X_blood_all_features, X_ecg_metadata, y_test_fusion, original_feature_names_list\n",
    "\n",
    "# # ============================================\n",
    "# # 4. PREDICTION GENERATION\n",
    "# # ============================================\n",
    "\n",
    "# def generate_predictions(X_blood_raw, X_ecg_metadata, y_true, original_feature_names):\n",
    "#     \"\"\"Generates P_A and P_B and creates the fusion dataset.\"\"\"\n",
    "    \n",
    "#     print(\"Generating Predictions\")\n",
    "#     print(f\"Original Feature List Size: {len(original_feature_names)}\")\n",
    "    \n",
    "#     # --- Load XGBoost Components (Model A) ---\n",
    "#     model_blood = joblib.load(BLOOD_MODEL_PATH)\n",
    "#     scaler = joblib.load(SCALER_PATH)\n",
    "#     selected_features = joblib.load(FEATURES_LIST_PATH)\n",
    "    \n",
    "#     # --- Load ECG Model (Model B) ---\n",
    "#     model_ecg = FinalMultiInputHybridModel(num_clinical_features=NUM_CLINICAL_FEATURES).to(DEVICE)\n",
    "#     model_ecg.load_state_dict(torch.load(ECG_MODEL_PATH, map_location=DEVICE))\n",
    "#     model_ecg.eval()\n",
    "    \n",
    "#     # --- A. Model A: XGBoost Predictions (P_A) ---\n",
    "#     X_test_blood_df = X_blood_raw.copy()\n",
    "    \n",
    "#     # 1. Align the columns and order them correctly for the scaler (99 features)\n",
    "#     X_test_full = X_test_blood_df[original_feature_names]\n",
    "    \n",
    "#     # Forcing NumPy conversion ensures no residual Pandas metadata confuses the scaler.\n",
    "#     X_test_full_np = X_test_full.values \n",
    "    \n",
    "#     print(f\"DEBUG CHECK: X_test_full_np shape: {X_test_full_np.shape}\")\n",
    "    \n",
    "#     # 2. Scale the full 99 features\n",
    "#     X_test_scaled_full = scaler.transform(X_test_full_np)\n",
    "    \n",
    "#     # 3. Select the final 30 features from the scaled array for the XGBoost model\n",
    "#     X_test_scaled_df = pd.DataFrame(X_test_scaled_full, columns=original_feature_names)\n",
    "#     X_test_sel = X_test_scaled_df[selected_features].values\n",
    "    \n",
    "#     # Predict probabilities\n",
    "#     P_A = model_blood.predict_proba(X_test_sel)\n",
    "#     print(f\"P_A (XGBoost) probabilities shape: {P_A.shape}\")\n",
    "\n",
    "#     # --- B. Model B: CNN-LSTM Predictions (P_B) ---\n",
    "#     ecg_dataset = HybridECGDataset(X_ecg_metadata)\n",
    "#     ecg_loader = DataLoader(ecg_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "#     P_B_list = []\n",
    "#     with torch.no_grad():\n",
    "#         for (x_signal, x_features), _ in tqdm(ecg_loader, desc=\"ECG Model Prediction\"):\n",
    "#             x_signal, x_features = x_signal.to(DEVICE), x_features.to(DEVICE)\n",
    "#             logits = model_ecg(x_signal, x_features)\n",
    "#             probabilities = F.softmax(logits, dim=1).cpu().numpy()\n",
    "#             P_B_list.append(probabilities)\n",
    "            \n",
    "#     P_B = np.concatenate(P_B_list, axis=0)\n",
    "#     print(f\"P_B (ECG-LSTM) probabilities shape: {P_B.shape}\")\n",
    "    \n",
    "#     # --- C. Create Fusion Dataset ---\n",
    "#     X_fusion = np.hstack((P_A, P_B)) # Shape N x 6\n",
    "#     Y_fusion = y_true\n",
    "    \n",
    "#     return X_fusion, Y_fusion\n",
    "\n",
    "# # ============================================\n",
    "# # 5. LATE FUSION (META-MODEL TRAINING)\n",
    "# # ============================================\n",
    "\n",
    "# def perform_late_fusion(X_fusion, Y_fusion):\n",
    "#     \"\"\"Trains a Logistic Regression Meta-Model on the combined probabilities.\"\"\"\n",
    "#     print(\"\\nTraining Logistic Regression Meta-Model\")\n",
    "    \n",
    "#     # Splitting the test set predictions into a Meta-Train and Meta-Test set\n",
    "#     X_meta_train, X_meta_test, y_meta_train, y_meta_test = train_test_split(\n",
    "#         X_fusion, Y_fusion, test_size=0.5, random_state=42, stratify=Y_fusion\n",
    "#     )\n",
    "\n",
    "#     # Train the Meta-Model (Logistic Regression)\n",
    "#     meta_model = LogisticRegression(solver='liblinear', multi_class='auto', random_state=42)\n",
    "#     meta_model.fit(X_meta_train, y_meta_train)\n",
    "    \n",
    "#     # --- SAVE THE FUSION MODEL CHECKPOINT ---\n",
    "#     os.makedirs(FUSION_DIR, exist_ok=True)\n",
    "#     FUSION_MODEL_FILENAME = os.path.join(FUSION_DIR, 'fusion_meta_model.joblib')\n",
    "    \n",
    "#     try:\n",
    "#         joblib.dump(meta_model, FUSION_MODEL_FILENAME)\n",
    "#         print(f\"\\nFusion Meta-Model successfully saved to {FUSION_MODEL_FILENAME}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error saving fusion model: {e}\")\n",
    "\n",
    "#     # Evaluate Fusion Performance on the Meta-Test Set\n",
    "#     y_fusion_pred = meta_model.predict(X_meta_test)\n",
    "    \n",
    "#     acc_fusion = accuracy_score(y_meta_test, y_fusion_pred)\n",
    "#     f1_w = f1_score(y_meta_test, y_fusion_pred, average='weighted')\n",
    "\n",
    "#     print(\"\\n\")\n",
    "#     print(\"FINAL LATE FUSION PERFORMANCE (Meta-Model)\")\n",
    "#     print(f\"Accuracy on Meta-Test Set: {acc_fusion:.4f}\")\n",
    "#     print(f\"Weighted F1-Score: {f1_w:.4f}\")\n",
    "#     print(\"\\nClassification Report:\")\n",
    "#     print(classification_report(y_meta_test, y_fusion_pred, target_names=['Low','Moderate','High']))\n",
    "#     print('\\n')\n",
    "\n",
    "#     # Optionally, you can also test the individual model performance on the same set for comparison\n",
    "#     P_A_test = X_meta_test[:, 0:NUM_CLASSES]\n",
    "#     P_B_test = X_meta_test[:, NUM_CLASSES:2*NUM_CLASSES]\n",
    "    \n",
    "#     acc_a = accuracy_score(y_meta_test, np.argmax(P_A_test, axis=1))\n",
    "#     acc_b = accuracy_score(y_meta_test, np.argmax(P_B_test, axis=1))\n",
    "#     print(f\"Individual XGBoost (A) Accuracy: {acc_a:.4f}\")\n",
    "#     print(f\"Individual ECG-LSTM (B) Accuracy: {acc_b:.4f}\")\n",
    "\n",
    "# # ============================================\n",
    "# # MAIN EXECUTION\n",
    "# # ============================================\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(f\"Starting fusion\")\n",
    "\n",
    "#     # Step 1: Load and Align the Common Test Data, Successfully loads the 99 feature names list\n",
    "#     X_blood_raw, X_ecg_metadata, y_test_fusion, original_feature_names_list = load_and_align_test_data(CLEANED_CSV_PATH, DUCKDB_PATH)\n",
    "\n",
    "#     # Step 2: Generate Predictions (P_A and P_B)\n",
    "#     X_fusion, Y_fusion = generate_predictions(X_blood_raw, X_ecg_metadata, y_test_fusion, original_feature_names_list)\n",
    "\n",
    "#     # Check if prediction generation was successful before fusion\n",
    "#     if X_fusion is not None:\n",
    "#         perform_late_fusion(X_fusion, Y_fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e67cb76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
